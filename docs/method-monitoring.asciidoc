[[java-method-monitoring]]
=== How to find slow methods

++++
<titleabbrev>Find slow methods</titleabbrev>
++++

Identifying a problematic service is only half of the battle when diagnosing application slowdowns.
Luckily, the Elastic APM Java Agent provides multiple ways to get method-level insights into your code.
This can help you diagnose slow requests due to heavy computations, inefficient algorithms,
or similar problems not related to interactions between services.

[float]
==== _If you don't know which methods you want to monitor..._

[float]
===== Sampling-based profiler

Find out which part of your code is making your application slow by
periodically recording running methods with a sampling-based profiler.

{y} Very low overhead. +
{y} No code changes required. +
{n} Does not work on Windows. +
{n} The duration of profiler-inferred spans are not exact measurements, only estimates.

<<method-sampling-based,Learn more>>

[float]
==== _If you know which methods you want to monitor..._

[float]
===== API/Code

Use the API or OpenTracing bridge to manually create spans for methods of interest.

{y} Most flexible. +
{n} Incorrect API usage may lead to invalid traces (scope leaks).

<<method-api,Learn more>>

[float]
===== Annotations

Annotations can be placed on top of methods to automatically create spans for them.

{y} Easier and more robust than the API. +
{n} Less flexible on its own, but can be combined with the API.

<<method-annotations,Learn more>>

[float]
===== Configuration-based

Use a configuration option to specify additional methods to instrument.

{y} No need to modify source code. +
{y} Possible to monitor code in third-party libraries. +
{y} Match methods via wildcards. +
{n} Easy to overuse which hurts runtime and startup performance.

<<method-config-based,Learn more>>

//********************************************
[[method-sampling-based]]
==== Sampling-based profiler

experimental::[]
added::[1.14.0]

Instead of recording every event, leverage the agent's built-in integration with
https://github.com/jvm-profiling-tools/async-profiler[async-profiler]
to periodically request the stack trace from all actively running threads.
This means measurements do not need to be inserted into all methods, which keeps the overhead of this approach extremely low.
Stack traces are then correlated with span activation events, and profiler-inferred spans for slow methods are created.
Just like that, we've detected exactly what is executing between current transactions and spans.

// Show simple example from Blog?
// BEFORE:: WelcomeController#welcome
// AFTER:: WelcomeController#think
// https://www.elastic.co/blog/from-distributed-tracing-to-distributed-profiling-with-elastic-apm

[float]
==== Use cases

* **Development**:
When trying to find out why the request you just made was slow.
* **Load testing / Production**:
When analyzing why some requests are slower than others.
* **Customer support**:
When a user complains that a particular request they made at noon was slow,
especially if you can't reproduce that slowness in your development or staging environment.

[float]
==== Advantages

* **No need to know what methods to monitor**:
Find slow methods without specifying specific method names up front.
The profiler automatically bubbles up slow methods as spans in the APM app.
* **Low overhead. Production ready**:
The profiler-based approach is designed to be low-overhead enough to run in production;
Continuously run it to provide insights into slow methods.

[float]
==== How to enable inferred spans with async-profiler

Enable inferred spans by setting <<config-profiling-inferred-spans-enabled,`profiling_inferred_spans_enabled`>> to `true`.

**Tune stack trace frequency**

Tune the frequency at which stack traces are gathered within a profiling session by adjusting
<<config-profiling-inferred-spans-sampling-interval,`profiling_inferred_spans_sampling_interval`>>.
The lower the sampling interval, the higher the accuracy and the level of detail of the inferred spans.
Of course, the higher the level of detail, the higher the profiler overhead and the Elasticsearch index sizes.
As most of the processing is done in the background, the impact on the response time of user requests is negligible.

**Clean up clutter in the APM app**

Filter out inferred spans that are faster than the configured threshold,
and avoid cluttering the APM app with fast-executing methods by setting
<<config-profiling-inferred-spans-min-duration,`profiling_inferred_spans_min_duration`>>.

**Include and exclude specific classes**

Include classes explicitly with <<config-profiling-inferred-spans-included-classes,`profiling_inferred_spans_included_classes`>>;
exclude with <<config-profiling-inferred-spans-excluded-classes,`profiling_inferred_spans_excluded_classes`>>.
Generally, the fewer classes that are included, the faster and the more memory efficient the processing is.

By default, the classes from the JDK and from most application servers are excluded. This reduces the number of uninteresting inferred spans.

**Example `elasticapm.properties` file with inferred spans enabled**

[source,properties]
----
profiling_inferred_spans_enabled=true
profiling_inferred_spans_sampling_interval=50ms
profiling_inferred_spans_min_duration=250ms
profiling_inferred_spans_included_classes=org.example.myapp.*
profiling_inferred_spans_excluded_classes=org.example.myapp.ignoreme.*
----

[float]
==== Caveats

Inferred spans are estimations, not exact measurements.
They may start after the method actually started, and end before the method actually ended.
This can lead to inconsistencies, all of which are documented in the
https://github.com/elastic/apm-agent-java/tree/master/apm-agent-plugins/apm-profiling-plugin[apm-profiling-plugin readme].

// Inferred spans are created after the profiling session ends and therefor may appear later in the APM app's span timeline than regular spans.
// Because of this, a regular span cannot be the child of an inferred span.

//********************************************
[[method-api]]
==== API/Code

Use the <<api-span,span API>> to manually create spans for methods of interest.
The API is extremely flexible, and offers the ability to customize your spans,
by adding labels to them, or by changing the type, name, or timestamp.

TIP: OpenTracing fan? You can use the <<opentracing-bridge,OpenTracing API>>, instead of the Agent API,
to manually create spans.

[float]
==== How to create spans with the span API

. Get the current span with <<api-current-span,`currentSpan()`>>,
which may or may not have been created with auto-instrumentation.
. Create a child span with <<api-span-start-span,`startSpan()`>>.
. Activate the span with <<api-span-activate,`activate()`>>.
. Customize the span with the <<api-span,span API>>.

[source,java]
----
import co.elastic.apm.api.ElasticApm;
import co.elastic.apm.api.Span;

Span parent = ElasticApm.currentSpan(); <1>
Span span = parent.startSpan(); <2>
try (Scope scope = span.activate()) { <3>
    span.setName("SELECT FROM customer"); <4>
    span.addLabel("foo", "bar"); <5>
    // do your thing...
} catch (Exception e) {
    span.captureException(e);
    throw e;
} finally {
    span.end();
}
----
<1> Get current span
<2> Create a child span
<3> Make this span the active span on the current thread
<4> Override the default span name
<5> Add labels to the span

[float]
==== Combine with annotations

// This section is sourced from the <<method-annotations>> documentation
include::method-monitoring.asciidoc[tag=combine-api-annotations]

//********************************************
[[method-annotations]]
==== Annotations

The <<api-annotation,annotation API>> allows you to place annotations on top of methods to automatically create spans for them.
This method of creating spans is easier, more robust, and typically more performant than using the API;
there’s nothing you can do wrong like forgetting to end a span or close a scope.

Annotations are less flexible when used on their own, but can be combined with the span API for added flexibility.

[float]
==== How-to create spans with the annotations API

Here's an example that uses the <<api-capture-span,`@CaptureSpan`>> annotation to create a span for the `spanWithAnnotation()` method.
The span is named `spanName`, is of type `ext`, and subtype `http`.

[source,java]
----
@CaptureSpan(value = "spanName", type = "ext", subtype = "http")
private static void spanWithAnnotation() {
    // do your thing...
}
----

[float]
==== Combine with the span API

// This content is also used in the `method-api` documentation
// Ensure any changes made here can also apply there.
// tag::combine-api-annotations[]
You can combine annotations with the span API to increase their flexibility.
Just get the current span on an annotated method and customize the span to your liking.

[source,java]
----
@CaptureSpan <1>
private static void spanWithAnnotation(String foo) {
    Span span = ElasticApm.currentSpan(); <2>
    span.setTag("foo", foo); <3>
}
----
<1> Use `@CaptureSpan` annotation to create a span
<2> Get the current span (the one created via the `@CaptureSpan` annotation)
<3> Customize the span
// end::combine-api-annotations[]

//********************************************
[[method-config-based]]
==== Configuration-based

Use the <<config-trace-methods,`trace_methods`>> configuration option to specify additional methods to instrument.
You can match methods via wildcards in the package, class or method name, by their modifier (like public),
by a particular annotation, and more.
Because you don’t need to modify your source code, this makes it possible to monitor code in 3rd party libraries.

Be careful, it's easy to overuse `trace_methods` by matching too many methods--hurting both runtime and startup performance.
Use in conjunction with <<config-trace-methods-duration-threshold,`trace_methods_duration_threshold`>> when setting for entire packages
in order to reduce overhead and avoid having too many spans in the APM app.

For more information, and examples, see the <<config-trace-methods,`trace_methods`>> configuration reference.
